{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G202158002 Shin Won Hyung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sigma: 0.1 / Gain seq: 1.0] mean terminal loss: 0.02096\n",
      "[Sigma: 0.1 / Gain seq: 0.501] mean terminal loss: 0.00046\n",
      "[Sigma: 1.0 / Gain seq: 1.0] mean terminal loss: 0.02539\n",
      "[Sigma: 1.0 / Gain seq: 0.501] mean terminal loss: 0.04935\n"
     ]
    }
   ],
   "source": [
    "def init_theta():\n",
    "    return np.ones([2])\n",
    "\n",
    "\n",
    "def compute_grad(theta):\n",
    "    t1, t2 = theta\n",
    "    grad_t1 = 4 * np.power(t1, 3) + 2 * t1 + t2\n",
    "    grad_t2 = t1 + 2 * t2\n",
    "    return np.array(grad_t1, grad_t2)\n",
    "\n",
    "\n",
    "def compute_loss(theta):\n",
    "    t1, t2 = theta\n",
    "    return np.power(t1, 4) + np.power(t1, 2) + t1 * t2 + np.power(t2, 2)\n",
    "\n",
    "\n",
    "def generate_noise(sigma):\n",
    "    return np.random.multivariate_normal(np.zeros([2]), np.identity(2) * sigma**2)\n",
    "\n",
    "\n",
    "def generate_gain_seq(iter, power_factor):\n",
    "    return 0.1 / np.power(iter + 1, power_factor)\n",
    "\n",
    "\n",
    "def update_theta(theta, grad, gain_seq):\n",
    "    return theta - gain_seq * grad\n",
    "\n",
    "\n",
    "max_iter = 1000\n",
    "max_repl = 200\n",
    "power_factor_list = [1., .501]\n",
    "sigma_list = [0.1, 1.0]\n",
    "\n",
    "for sigma in sigma_list:\n",
    "\n",
    "    for power_factor in power_factor_list:\n",
    "        results = []\n",
    "\n",
    "        for seed in range(max_repl):\n",
    "            np.random.seed(seed)\n",
    "            theta = init_theta()\n",
    "\n",
    "            for iter in range(1, max_iter+1): \n",
    "                noise = generate_noise(sigma)\n",
    "                grad = compute_grad(theta) + noise\n",
    "                gain_seq = generate_gain_seq(iter, power_factor)\n",
    "                theta = update_theta(theta, grad, gain_seq)\n",
    "\n",
    "            terminal_loss = compute_loss(theta)\n",
    "            results.append(terminal_loss)\n",
    "        \n",
    "        print(f\"[Sigma: {sigma} / Gain seq: {power_factor}] mean terminal loss: {np.round(np.mean(results), 5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Independent Noise / Replication 1] theta = [2.20192136 0.49681287]\n",
      "[Independent Noise / Replication 2] theta = [2.2039089  0.76719299]\n",
      "[Independent Noise / Replication 3] theta = [1.38861336 3.69005965]\n",
      "[Independent Noise / Replication 4] theta = [2.17752456 0.84727258]\n",
      "[Independent Noise / Replication 5] theta = [-0.11949166  4.08707692]\n",
      "[Dependent Noise / Replication 1] theta = [2.08324208 0.58325698]\n",
      "[Dependent Noise / Replication 2] theta = [2.04769971 0.62232833]\n",
      "[Dependent Noise / Replication 3] theta = [2.07071476 0.58293668]\n",
      "[Dependent Noise / Replication 4] theta = [2.06380728 0.65899708]\n",
      "[Dependent Noise / Replication 5] theta = [2.07745896 0.63604654]\n"
     ]
    }
   ],
   "source": [
    "def generate_x_pairs(max_sample_num=1000):\n",
    "    c = np.random.randint(1, 11, size=max_sample_num)\n",
    "    w = np.random.randint(11, 111, size=max_sample_num)\n",
    "    return np.stack([w,c], 1)\n",
    "\n",
    "def init_theta():\n",
    "    return np.array([1., .5])\n",
    "\n",
    "\n",
    "def generate_gain_seq(iter):\n",
    "    return 0.0015 / np.power(iter + 100, .501) \n",
    "\n",
    "\n",
    "def generate_noise_1():\n",
    "    return np.random.normal(0, 5**2)\n",
    "\n",
    "\n",
    "def generate_noise_2(optimal_theta, x):\n",
    "    h = compute_h(optimal_theta, x)\n",
    "    w = np.random.normal(0, 1)\n",
    "    return 0.2 * h * w\n",
    "\n",
    "\n",
    "def compute_h(theta, x):\n",
    "    lmbd, beta = theta\n",
    "    w, c = x\n",
    "    return lmbd * np.power(c, beta) * np.power(w, 1-beta)\n",
    "\n",
    "\n",
    "def compute_euclidian(theta, optimal_theta):\n",
    "    return np.sqrt(np.sum(np.power(theta - optimal_theta, 2)))\n",
    "\n",
    "\n",
    "def compute_grad(theta, x):\n",
    "    lmbd, beta = theta\n",
    "    w, c = x\n",
    "    grad_lmbd = w * np.power(c / w, beta)\n",
    "    grad_beta = lmbd * w * np.power(c / w, beta) * np.log(c / w)\n",
    "    \n",
    "    return np.array([grad_lmbd, grad_beta])\n",
    "\n",
    "\n",
    "def update_theta(theta, gain_seq, grad, x, z):\n",
    "    h = compute_h(theta, x)\n",
    "    return theta - gain_seq * grad * (h - z)\n",
    "\n",
    "\n",
    "max_iter = 1000\n",
    "max_repl = 5\n",
    "optimal_theta = np.array([2.5, 0.7])\n",
    "\n",
    "x = generate_x_pairs()\n",
    "for noisy_type in [\"Independent\", \"Dependent\"]:\n",
    "\n",
    "    for seed in range(max_repl):\n",
    "        theta = init_theta()\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(x)\n",
    "        \n",
    "        for iter, x_sample in enumerate(x):\n",
    "            gain_seq = generate_gain_seq(iter+1)\n",
    "            \n",
    "            if noisy_type == \"Independent\":\n",
    "                noise = generate_noise_1()\n",
    "            else:\n",
    "                noise = generate_noise_2(optimal_theta, x_sample)\n",
    "\n",
    "            z = compute_h(optimal_theta, x_sample) + noise\n",
    "            grad = compute_grad(theta, x_sample)\n",
    "\n",
    "            theta = update_theta(theta, gain_seq, grad, x_sample, z)\n",
    "        print(f\"[{noisy_type} Noise / Replication {seed+1}] theta = {theta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\theta^* = [0,0]^T$, $g(\\theta^*) = [0,0]^T$\n",
    "\n",
    "According to statistics conditions for convergence A.2,\n",
    "$$\\inf_{\\eta < ||\\theta||< 1 / \\eta} \\theta^T B g(\\theta) > 0, \\quad \\text{where} ~ 0 < \\eta < 1.$$\n",
    "\n",
    "Let $\\theta = [0,1]^T$, then $\\forall 0 < \\eta < 1, ~ \\eta < ||\\theta|| < 1 / \\eta$ is satisfied.\n",
    "\n",
    "If $B = I_2$, then\n",
    "\n",
    "$$\\theta^T B g(\\theta) = \\theta^T g(\\theta) = 0 - 0 - 0 + \\frac{1}{2} + 1 - 2 + 0 - 0 = -\\frac{1}{2} < 0.$$\n",
    "\n",
    "$\\therefore$ $g(\\theta)$ fails convergence condition A.2 when $B = I_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Standard] Terminal loss: 0.0364 ± 0.045\n",
      "[Iterative Averaging recent 100] Terminal loss: 0.0369 ± 0.0451\n",
      "[Iterative Averaging] Terminal loss: 0.0292 ± 0.0356\n"
     ]
    }
   ],
   "source": [
    "def init_theta():\n",
    "    return np.ones([2])\n",
    "\n",
    "\n",
    "def compute_grad(theta):\n",
    "    t1, t2 = theta\n",
    "    grad_t1 = 4 * np.power(t1, 3) + 2 * t1 + t2\n",
    "    grad_t2 = t1 + 2 * t2\n",
    "    return np.array(grad_t1, grad_t2)\n",
    "\n",
    "\n",
    "def compute_loss(theta):\n",
    "    t1, t2 = theta\n",
    "    return np.power(t1, 4) + np.power(t1, 2) + t1 * t2 + np.power(t2, 2)\n",
    "\n",
    "\n",
    "def generate_noise(sigma):\n",
    "    return np.random.multivariate_normal(np.zeros([2]), np.identity(2) * sigma**2)\n",
    "\n",
    "\n",
    "def generate_gain_seq(iter):\n",
    "    return 0.1 / np.power(iter + 1, 0.501)\n",
    "\n",
    "\n",
    "def update_theta(theta, grad, gain_seq):\n",
    "    return theta - gain_seq * grad\n",
    "\n",
    "\n",
    "max_iter = 1000\n",
    "max_repl = 50\n",
    "sigma = 1.0\n",
    "\n",
    "results = []\n",
    "for seed in range(max_repl):\n",
    "    np.random.seed(seed)\n",
    "    theta = init_theta()\n",
    "\n",
    "    theta_list = [theta]\n",
    "    for iter in range(1, max_iter+1): \n",
    "        noise = generate_noise(sigma)\n",
    "        grad = compute_grad(theta) + noise\n",
    "        gain_seq = generate_gain_seq(iter)\n",
    "        theta = update_theta(theta, grad, gain_seq)\n",
    "        theta_list.append(theta)\n",
    "\n",
    "    standard_loss = compute_loss(theta)\n",
    "    iter100_avg_loss = compute_loss(np.mean(theta_list[-100:], axis=0))\n",
    "    iter_avg_loss = compute_loss(np.mean(theta_list, axis=0))\n",
    "    results.append(np.array([standard_loss, iter100_avg_loss, iter_avg_loss]))\n",
    "\n",
    "\n",
    "results_arr = np.stack(results, 0)\n",
    "experiments = [\"Standard\", \"Iterative Averaging recent 100\", \"Iterative Averaging\"]\n",
    "for i in range(3):\n",
    "    print(f\"[{experiments[i]}] Terminal loss: {np.round(np.mean(results_arr[:,i]), 4)} ± {np.round(np.std(results_arr[:,i], ddof=1), 4)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
